{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2679eb14cb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "\n",
    "from nano_medgpt.definitions import *\n",
    "from nano_medgpt.src.datasets.utils import *\n",
    "from nano_medgpt.src.models.bigram import BigramLM\n",
    "from nano_medgpt.src.models.gpt import *\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from azureml.core.run import Run\n",
    "import glob\n",
    "import os\n",
    "\n",
    "torch.manual_seed(456123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamters\n",
    "vocab_size = 37\n",
    "n_epochs = 100\n",
    "eval_iters = 10\n",
    "eval_interval = 10\n",
    "batch_size = 64\n",
    "context_length = 256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(m, max_new_tokens=100):\n",
    "    idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    generated_idx = m.generate(idx, max_new_tokens=max_new_tokens)[0].tolist()\n",
    "    return decode(generated_idx)\n",
    "\n",
    "def _get_batch(df):\n",
    "    ix = torch.randint(len(df)-context_length, (batch_size,))\n",
    "    x = torch.stack([df[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([df[i+1:i+context_length+1] for i in ix])\n",
    "    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_batch(data_folder=None, split=\"train\"):\n",
    "    part_selected = 0\n",
    "    if split == \"train\":\n",
    "        part_selected = 0 # random.randint(0, 5)\n",
    "    else:\n",
    "        part_selected = 6\n",
    "    # fpath = data_folder + \"part_\" + str(part_selected) + \"_encoded.pt\"\n",
    "    # df = torch.load(enc)\n",
    "    fpath = data_folder + \"part\\\\\" + str(part_selected) + \"\\\\chunk_0.pkl\"\n",
    "    with open(fpath, 'rb') as f:\n",
    "        enc = pickle.load(f)\n",
    "    df = torch.tensor(enc, dtype=torch.long)\n",
    "    return _get_batch(df)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(data_folder, model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(data_folder = data_folder, split=split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train_model(data_folder = None, make_encodings=False):\n",
    "    if make_encodings:\n",
    "        make_encodings()\n",
    "\n",
    "    # instantiate the model and the optimizer\n",
    "    model = GPT()\n",
    "    m = model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    best_val_loss = 1e9\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # First, let's check to see if we are at an eval step\n",
    "        if i % eval_interval == 0:\n",
    "            losses = estimate_loss(data_folder, model)\n",
    "            print(\"Step number {}: Training Loss: {}, Validation Loss:{}\".format(i, losses['train'], losses['val']))\n",
    "        \n",
    "        # Check for model saving condition\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            # torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "            torch.save(model.state_dict(), './outputs/model.pth')\n",
    "\n",
    "        # Main training loop\n",
    "        xb, yb = get_batch(data_folder=data_folder, split=\"train\")\n",
    "        logits, loss = m(xb, yb)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    print(\"After training, the best validation loss is {}\".format(loss.item()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number 0: Training Loss: 3.8526923656463623, Validation Loss:3.8889126777648926\n",
      "Step number 10: Training Loss: 3.8736984729766846, Validation Loss:3.862678050994873\n",
      "Step number 20: Training Loss: 3.8838753700256348, Validation Loss:3.8779869079589844\n",
      "Step number 30: Training Loss: 3.842189073562622, Validation Loss:3.8719208240509033\n",
      "Step number 40: Training Loss: 3.86883282661438, Validation Loss:3.844949722290039\n",
      "Step number 50: Training Loss: 3.8267223834991455, Validation Loss:3.87621808052063\n",
      "Step number 60: Training Loss: 3.854963779449463, Validation Loss:3.8019726276397705\n",
      "Step number 70: Training Loss: 3.7992324829101562, Validation Loss:3.834172010421753\n",
      "Step number 80: Training Loss: 3.7863292694091797, Validation Loss:3.8002943992614746\n",
      "Step number 90: Training Loss: 3.7799301147460938, Validation Loss:3.794801712036133\n",
      "After training, the best validation loss is 3.770153760910034\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--data-folder', type=str, help='dataset')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# os.makedirs('./outputs', exist_ok=True)\n",
    "# data_folder = args.data_folder\n",
    "\n",
    "# run = Run.get_context() \n",
    "data_folder = \"C:\\\\Users\\\\Ben\\\\git\\\\nano_medgpt\\\\data\\\\interim\\\\encodings\\\\\"\n",
    "\n",
    "model = train_model(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s5zrhbw mbriw88xzug 1ww41iz8bxtnx8acjazhxzrpfn4tp6n37gz cmp8hzxz801u1qlrew4fhw5r8xuugw8oer3rj963edr6\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano-medgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
